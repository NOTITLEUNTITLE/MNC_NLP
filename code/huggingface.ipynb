{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be3c62f-d622-4271-87be-1dd64407f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8534e9-227e-412c-8416-661344a93684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>bias</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"'미스터 샤인션' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"</td>\n",
       "      <td>김태리 정말 연기잘해 진짜</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...</td>\n",
       "      <td>공효진 발연기나이질생각이읍던데 왜계속주연일까</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"</td>\n",
       "      <td>누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...</td>\n",
       "      <td>others</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"</td>\n",
       "      <td>일본 축구 져라</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...</td>\n",
       "      <td>난 절대로 임현주 욕하는인간이랑은 안논다 @.@</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0         \"'미스터 샤인션' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"   \n",
       "1  \"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...   \n",
       "2             \"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"   \n",
       "3               \"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"   \n",
       "4  \"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...   \n",
       "\n",
       "                                             comment    bias  hate  \n",
       "0                                     김태리 정말 연기잘해 진짜    none  none  \n",
       "1                           공효진 발연기나이질생각이읍던데 왜계속주연일까    none  hate  \n",
       "2  누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...  others  hate  \n",
       "3                                           일본 축구 져라    none  none  \n",
       "4                         난 절대로 임현주 욕하는인간이랑은 안논다 @.@    none  none  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH =  '/USER/3_WEEK/MNC_NLP/'\n",
    "\n",
    "train = pd.read_csv(os.path.join(PATH, 'train/train.csv'), encoding='utf-8')\n",
    "test = pd.read_csv(os.path.join(PATH, 'test/test.csv'), encoding='utf-8')\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f72d187f-d13b-44f9-aba4-46381603a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8367 entries, 0 to 8366\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    8367 non-null   object\n",
      " 1   comment  8367 non-null   object\n",
      " 2   bias     8367 non-null   object\n",
      " 3   hate     8367 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 261.6+ KB\n",
      "None\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 511 entries, 0 to 510\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   ID       511 non-null    int64 \n",
      " 1   title    511 non-null    object\n",
      " 2   comment  511 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 12.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train.info(), end='\\n\\n')\n",
    "print(test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eec1cd4-98b8-4181-949a-753f5a54193c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Columns:  Index(['title', 'comment', 'bias', 'hate'], dtype='object')\n",
      "Test Columns:  Index(['ID', 'title', 'comment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Train Columns: ', train.columns)\n",
    "print('Test Columns: ', test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16900ef4-9fef-4e57-afeb-75ec20c99897",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = np.array(np.meshgrid(train.bias.unique(), train.hate.unique())).T.reshape(-1,2)\n",
    "bias_hate = list(np.array([train['bias'].values, train['hate'].values]).T.reshape(-1,2))\n",
    "labels = []\n",
    "for i, arr in enumerate(bias_hate):\n",
    "    for idx, elem in enumerate(combinations):\n",
    "        if np.array_equal(elem, arr):\n",
    "            labels.append(idx)\n",
    "\n",
    "train['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c19f64bd-037c-4d62-82a3-e97bfbf13df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train)\n",
    "train = train[:-367]\n",
    "test = train[-367:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "667d0731-3b33-475c-8fb4-14123882110b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Label: \n",
      "0    3278\n",
      "1    1974\n",
      "3    1371\n",
      "5    1163\n",
      "2     134\n",
      "4      80\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Test Label: \n",
      "0    140\n",
      "1     91\n",
      "3     71\n",
      "5     52\n",
      "4      7\n",
      "2      6\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Train Label: ', train['label'].value_counts(), sep='\\n', end='\\n\\n')\n",
    "print('Test Label: ', test['label'].value_counts(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02013ed0-7ca0-45c3-a06e-d993867c97e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Null: \n",
      "title      0\n",
      "comment    0\n",
      "bias       0\n",
      "hate       0\n",
      "label      0\n",
      "dtype: int64\n",
      "\n",
      "Test Null: \n",
      "title      0\n",
      "comment    0\n",
      "bias       0\n",
      "hate       0\n",
      "label      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Train Null: ', train.isnull().sum(), sep='\\n', end='\\n\\n')\n",
    "print('Test Null: ', test.isnull().sum(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a83fda0c-cd20-4c09-9a8e-c4b8efaf8e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature = train['label']\n",
    "\n",
    "plt.figure(figsize=(10,7.5))\n",
    "plt.title('Train dataset', fontsize=20)\n",
    "\n",
    "temp = feature.value_counts()\n",
    "\n",
    "font1 = {\n",
    "    'color':  'red',\n",
    "    'weight': 'bold',\n",
    "    'size': 17,\n",
    "}\n",
    "# print(type(temp))\n",
    "# temp = dict(temp)\n",
    "# print(temp)\n",
    "plt.bar(temp.keys(), temp.values, width=0.5, color='b', alpha=0.5)\n",
    "\n",
    "plt.text(-0.05, temp.values[0]+20, s=temp.values[0],fontdict=font1)\n",
    "plt.text(0.95, temp.values[1]+20, s=temp.values[1],fontdict=font1)\n",
    "plt.text(1.95, temp.values[4]+20, s=temp.values[4],fontdict=font1)\n",
    "plt.text(2.95, temp.values[2]+20, s=temp.values[2],fontdict=font1)\n",
    "plt.text(3.95, temp.values[5]+20, s=temp.values[5],fontdict=font1)\n",
    "plt.text(4.95, temp.values[3]+20, s=temp.values[3],fontdict=font1)\n",
    "\n",
    "plt.xticks(temp.keys(), fontsize=12) # x축 값, 폰트 크기 설정\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # 레이아웃 설정\n",
    "plt.show() # 그래프 나타내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f77a448a-d55e-49f5-99f9-0c31c35f74c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature = test['label']\n",
    "\n",
    "plt.figure(figsize=(10,7.5))\n",
    "plt.title('Train dataset', fontsize=20)\n",
    "\n",
    "temp = feature.value_counts()\n",
    "\n",
    "font1 = {\n",
    "    'color':  'red',\n",
    "    'weight': 'bold',\n",
    "    'size': 17,\n",
    "}\n",
    "# print(type(temp))\n",
    "# temp = dict(temp)\n",
    "# print(temp)\n",
    "plt.bar(temp.keys(), temp.values, width=0.5, color='b', alpha=0.5)\n",
    "\n",
    "plt.text(-0.05, temp.values[0]+20, s=temp.values[0],fontdict=font1)\n",
    "plt.text(0.95, temp.values[1]+20, s=temp.values[1],fontdict=font1)\n",
    "plt.text(1.95, temp.values[4]+20, s=temp.values[4],fontdict=font1)\n",
    "plt.text(2.95, temp.values[2]+20, s=temp.values[2],fontdict=font1)\n",
    "plt.text(3.95, temp.values[5]+20, s=temp.values[5],fontdict=font1)\n",
    "plt.text(4.95, temp.values[3]+20, s=temp.values[3],fontdict=font1)\n",
    "\n",
    "plt.xticks(temp.keys(), fontsize=12) # x축 값, 폰트 크기 설정\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # 레이아웃 설정\n",
    "plt.show() # 그래프 나타내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffac11c8-db5c-49da-9590-68b8a404ca5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Premise Length:  63\n",
      "Min Premise Length:  2\n",
      "Mean Premise Lenght:  42.46475 \n",
      "\n",
      "Max Hypothesis Length:  137\n",
      "Min Hypothesis Length:  4\n",
      "Mean Hypothesis Lenght:  38.710625\n"
     ]
    }
   ],
   "source": [
    "max_len = np.max(train['title'].str.len())\n",
    "min_len = np.min(train['title'].str.len())\n",
    "mean_len = np.mean(train['title'].str.len())\n",
    "\n",
    "print('Max Premise Length: ', max_len)\n",
    "print('Min Premise Length: ', min_len)\n",
    "print('Mean Premise Lenght: ', mean_len, '\\n')\n",
    "\n",
    "max_len = np.max(train['comment'].str.len())\n",
    "min_len = np.min(train['comment'].str.len())\n",
    "mean_len = np.mean(train['comment'].str.len())\n",
    "\n",
    "print('Max Hypothesis Length: ', max_len)\n",
    "print('Min Hypothesis Length: ', min_len)\n",
    "print('Mean Hypothesis Lenght: ', mean_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d29fc8ca-13d7-4de8-b262-b93c76513c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "plt.figure(figsize=(10,7.5))\n",
    "plt.title('Train title Length', fontsize=20)\n",
    "\n",
    "plt.hist(train['title'].str.len(), alpha=0.5, color='orange')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # 레이아웃 설정\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3458cff-afec-43b1-bf5d-ced22940ada7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAHpCAYAAACfs8p4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAfZElEQVR4nO3de7hvdV0n8PdHTlJaD5c4EgF6TGmUDNEh02wm1EIwC+1iOJVI9uDMUGk19ag95SnHyakmzS6OpAQ2Jvl4SUq8MCSp03g5eDkqaDCGAiKgCJoUhn7nj7V2ftrszTlnXw/nvF7P83vW7/dd37XWd333Ome/f2t/11o1xggAADC522Y3AAAA9iYCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjKwaapqVNUlm90OWGuObbhrE5BhPzb/Et+T11M3u83sParqkqra45vpV9VT5+Pp3HVo1oaoqquq6qrNbgewPrZsdgOATfXrS5Q9M8lBSX4vyc2L5n1gjbf/wCS3rvE6AWBVBGTYj40xti8um88SH5TkRWOMq9Z5+x9dz/UDwEoYYgHsloU/p1fV3avq16rqY1V128KfyavqoKr6par666q6pqq+VFU3VtUFVfWIZdZ5h3GaVbV9Lj+xqn6kqt5TVbdW1U1VdX5VHbmCtv9YVV08r+Of5j+Pv6qqTlhU78CqelZVfWje5uer6h1V9aQl1rltYZhAVd2vql5TVZ+tqi9U1Vur6kFzva1VdXZVXTdv+71V9agl1tf3+8lVdenchk9V1e9W1YFzvUfPP4vPV9XnqupPq+obl9nvo6rqD6rq4/PP6rPzz+M7drH9O+33hX1P8j3z5z4M55I9++nsWlUdWlW/WVWXV9U/VtUt88/zpCXqLgzfeGpVPWruqy/M/fXGqnrgMtv41qp67dynX6yqv62q7+/rm+udOO/7fZLcZ9G+n7vEeg9rP//bquojVXXG2vYQsNacQQb21GuTfEeSNyX5iyQ3zOUPTPL8JG9P8sYkn0ty7yQ/mOSUqvqBMcab92A7/3le9oIkf5PkO5P8WJIHV9XxY4zbdrWCqqokf5Lk9CSfSfK6JDcmOSrJo5J8LMmOue7dk7wlU+j7aJI/THKPJD+S5M/nbT5nic1sS/LuJJcnOXf+/MQkl8xfDN6c5PNJ/jzJoUlOS/KmqvrWMcYnl1jfzyY5JVPfXpLkpCQ/n+TQqnpDkvMz9e/ZSb4ryU8kOWxepu/7Q5O8dd7mW+Z9PyzJE5K8s6qeOMa4cInt706/35xpeM5TMwXFPlTnqiXWuWJVdZ9M/bAtyTsy9ec9kzw+yZur6uljjD9eYtHHJzk103H6P5Mcm+RxSb6jqo4dY3ymbeMBSf42ySGZ+nZnkm9J8voki/voqkz7+8z584vavA8sqntwkv+T5EtJXpPkwCQ/muScqvrKGOO8Xe0/sEnGGF5eXl7/8soUAEaSbYvKL5nLdyY5bInlDlqm/Kgkn0py+RLzRpJLFpVtn8s/n+TbF837s3nek3ZzX86c678nyUGL5h2Q5Ij2+dlz3QuTbGnl92p98l2tfNtcNpL8yqJ1/+pcflOmcHa3Nu8n53kvXGa/b0nywFZ+YJKPJPlyks8m+Z42725JLpqXO76Vb0lyZZJ/6vXned+c5Nok1yU5cDX9vnBMrOAYe+q8vnN3o+4lSb6S5LRF5QdnCqT/mOTwJdZ9e5LHLFrmN+d5v7yo/OK5/D8tKj+l/YyfusS/k6vupN0Ly70syQGt/Ni5bZet9N+ol5fX+r8MsQD21K+OdvZtwRjjlmXKr8l09uwBVXXvPdjOi8cYH1pUtnCm8GG7uY6fnadPH2PcsqhdXx5jXNeKfipToPmFMcbtrd4NSZ43f/zpJbZxVZIXLCpbODN4YJJfGmN8pc37s0wB6fhl2vziMcblbfu3ZTr7fLckbxxj/E2b95Uk/2v++OC2ju9Pcr8kv9/rz8t8KslvJfmmJI9ZZvur7fc1UVUPznRG/7VjjPP7vDHGzUmem+Rrk/zwEoufP8a4eFHZ2fP0X/ajqo5O8uhMXyheumgbb0ryv1exC7dmOp6+3NZ5Waazyg+sqq9fxbqBdWSIBbCn3rPcjKp6ZJJnJHlEpjOvd19U5cgkSw0rWMqOJcqunqeH7GrhqrpnkgcluX6M8f5d1P2GJPdPcu1Y+sLBv56nD1li3gd6AJp9ap7+3RjjC33GGOPLVXV9pjPrS1lqvxfWd+kS866dp319C2O+71NV25dY5ph5+sDccQjBqvp9jS3sx0HL7MfWebrUuOLd3Y/j5+n/XfRFZsE7k3zvnTdzWVeMMT6/i3b8wwrXDawjARnYU59eqrCqnpjpTPE/Zfqz//9L8sVMfx4/MdOZwAP3YDs3L1G2cGb3gN1Y/uB5eu2dVZodNE+vW2b+QvnBS8y7ZXHBGOP2afjzHefNbk/yNcvMW2qZ23djXl/fwkV7P7rMNhYsdQbz5jvZxu70+1pa2I/vm1/L2a39aD+Xvh8LP/vrl1n3cuW74w5tmG1WfwK7SUAG9sgYY7kHQzwv08VIJ/QhAklSVS/NfMeDDXTzPN2du14sBM9vWmb+EYvq7e0W2nnqGOOCTW3J6izsxzPGGC9ep20snOE9fJn5y5UD+zBjkIG1cv9MFx4tDsd3S/LdG92YMcYXk3w4yeFVtdTQiF73C5nOeB9ZVccsUWXhtmzvW9tWrpt3zdN/t87b+XKSVNV6nQndiP34wDx9xHysLrbcsfvlOAMM+ywBGVgrVyU5pqq+eaFgvs3a9kxX7m+GhbOOL62qg/qMqrpbVR3Ris5JUkl+uwe+qjos010pFurcFbwhU+A/q6oet1SFqnpEVd1jldv57Dzdk4svd9sYY0emW7v9UFX91FJ1qurbq+peq9jGJzPdKeP+SZ6+aN0nZ/nxx59NsrWqvm6l2wb2XoZYAGvlhZluafb+qnptkn9O8shM4fgvk/zAJrTpZZnOPv5kkivm+wjfmOlWZ4/OFHi3z3V/J9NtvU5N8sGqujDTfZB/NNMFh781xnjnhrZ+hcYY/1xVP5Tp/sdvrKq/zXSm9NYkR2e6j/W3ZBo6sppHfV+cqX9eN/fXPyb5xBjjT3dz+e9e6uEas/fNwyr+Q6aLJF9eVT+X6Z7TN2e6KPG4TBdiPiJfvR/3SpyV6c4SfzR/oVi4D/IPZ/qycWqmsfTdxZn68c1V9fYktyX54BjjL1fRDmAvISADa2KM8dKqui3TAxROzxSW3pHkjExBY8MD8jxe+ilV9ZZM90R+UqYLBa+b23ZBq/ulqvq+JL+QKZT9bKaLqT6Y5JljjFdtcPNXZYyxc75N2i9kemjGGZlC3nVJ3p/pFml3uC3fHnpZpgeFnJbklzP9TvmbJLsbkO83v5ZycKZbzl1TVf8208/jh5P8eKahDZ9OclmS30+y+LZ0e2SMcdn8UJf/lumL06MzheQnZrpDxqn56ljlBf91buMPZPoieECm2/sJyLAPqOWvtwGA/VtVvTLTF6YHjDE+ttntATaGMcgA7Nfm8eh3uINJVT0m02O2LxOOYf9iiAUA+7u7J7m6qt6W5KOZhtZ8W6Z7L38p0xhlYD9iiAUA+7X5riUvyjT2+KhMF2d+Jsnbk7xgV09iBPY9AjIAADTGIAMAQLNXj0E+7LDDxrZt2za7GQAA3AVdeumlnxljbN3T5fbqgLxt27bs2LFjs5sBAMBdUFV9YiXLGWIBAACNgAwAAI2ADAAAjYAMAACNgAwAAI2ADAAAjYAMAACNgAwAAI2ADAAAjYAMAACNgAwAAI2ADAAAjYAMAACNgAwAAI2ADAAAjYAMAACNgAwAAI2ADAAAzZbNbgB7gZ3bN7sFG+u47ZvdAgBgL+YMMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADS7DMhVdXRVva2qLquqj1TVM+byQ6vqoqq6Yp4eMpdXVb24qq6sqp1V9dC2rtPn+ldU1enrt1sAALAyu3MG+fYkvzjGODbJw5OcVVXHJnlWkovHGMckuXj+nCSnJDlmfp2Z5CXJFKiTPDfJdyZ5WJLnLoRqAADYW+wyII8xrhtjvG9+/4Uklyc5MsmpSc6bq52X5Anz+1OTvGJM3pXk4Ko6Isljk1w0xrhpjPG5JBclOXktdwYAAFZrj8YgV9W2JA9J8u4kh48xrptnfTrJ4fP7I5Nc3Ra7Zi5brhwAAPYaux2Qq+rrk7w2yTPHGJ/v88YYI8lYiwZV1ZlVtaOqdtx4441rsUoAANhtuxWQq+prMoXjV44xXjcXXz8Pncg8vWEuvzbJ0W3xo+ay5cr/lTHG2WOME8YYJ2zdunVP9gUAAFZtd+5iUUlenuTyMcbvtlkXJFm4E8XpSd7Qyp8y383i4UlumYdivCXJSVV1yHxx3klzGQAA7DW27EadRyb5ySQfqqoPzGXPSfKCJK+uqqcl+USSJ83zLkzyuCRXJrk1yRlJMsa4qaqel+S9c73fGGPctBY7AQAAa2WXAXmM8c4ktczsxyxRfyQ5a5l1nZPknD1pIAAAbCRP0gMAgEZABgCARkAGAIBGQAYAgEZABgCARkAGAIBGQAYAgEZABgCARkAGAIBGQAYAgEZABgCARkAGAIBGQAYAgEZABgCARkAGAIBGQAYAgEZABgCARkAGAIBGQAYAgEZABgCARkAGAIBGQAYAgEZABgCARkAGAIBGQAYAgEZABgCARkAGAIBGQAYAgEZABgCARkAGAIBGQAYAgEZABgCARkAGAIBGQAYAgEZABgCARkAGAIBGQAYAgEZABgCARkAGAIBGQAYAgEZABgCARkAGAIBGQAYAgGbLZjdgr7Rz+2a3AACATeIMMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0AjIAADQCMgAANAIyAAA0uwzIVXVOVd1QVR9uZdur6tqq+sD8elyb9+yqurKqPlZVj23lJ89lV1bVs9Z+VwAAYPV25wzyuUlOXqL8hWOM4+fXhUlSVccmOS3Jt83L/FFVHVBVByT5wySnJDk2yZPnugAAsFfZsqsKY4y3V9W23VzfqUnOH2PcluTvq+rKJA+b5105xvh4klTV+XPdy/a8yQAAsH5WMwb5Z6pq5zwE45C57MgkV7c618xly5XfQVWdWVU7qmrHjTfeuIrmAQDAnltpQH5JkvslOT7JdUn+x1o1aIxx9hjjhDHGCVu3bl2r1QIAwG7Z5RCLpYwxrl94X1V/nOSv5o/XJjm6VT1qLsudlAMAwF5jRWeQq+qI9vGJSRbucHFBktOq6sCqum+SY5K8J8l7kxxTVfetqrtnupDvgpU3GwAA1scuzyBX1auSnJjksKq6Jslzk5xYVccnGUmuSvL0JBljfKSqXp3p4rvbk5w1xvjyvJ6fSfKWJAckOWeM8ZG13hkAAFit3bmLxZOXKH75ndR/fpLnL1F+YZIL96h1AACwwTxJDwAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAAJoVPUkP7tJ2bt/sFmys47ZvdgsA4C7FGWQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaHYZkKvqnKq6oao+3MoOraqLquqKeXrIXF5V9eKqurKqdlbVQ9syp8/1r6iq09dndwAAYHV25wzyuUlOXlT2rCQXjzGOSXLx/DlJTklyzPw6M8lLkilQJ3luku9M8rAkz10I1QAAsDfZZUAeY7w9yU2Lik9Nct78/rwkT2jlrxiTdyU5uKqOSPLYJBeNMW4aY3wuyUW5Y+gGAIBNt9IxyIePMa6b3386yeHz+yOTXN3qXTOXLVd+B1V1ZlXtqKodN9544wqbBwAAK7Pqi/TGGCPJWIO2LKzv7DHGCWOME7Zu3bpWqwUAgN2y0oB8/Tx0IvP0hrn82iRHt3pHzWXLlQMAwF5lpQH5giQLd6I4PckbWvlT5rtZPDzJLfNQjLckOamqDpkvzjtpLgMAgL3Kll1VqKpXJTkxyWFVdU2mu1G8IMmrq+ppST6R5Elz9QuTPC7JlUluTXJGkowxbqqq5yV571zvN8YYiy/8AwCATbfLgDzGePIysx6zRN2R5Kxl1nNOknP2qHUAALDBPEkPAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAAJpdPigEuIvbuX2zW7Cxjtu+2S0A4C7OGWQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABotmx2AwDW1M7tm92CjXXc9s1uAcA+xxlkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGgEZAAAaARkAABoBGQAAGhWFZCr6qqq+lBVfaCqdsxlh1bVRVV1xTw9ZC6vqnpxVV1ZVTur6qFrsQMAALCW1uIM8qPGGMePMU6YPz8rycVjjGOSXDx/TpJTkhwzv85M8pI12DYAAKyp9RhicWqS8+b35yV5Qit/xZi8K8nBVXXEOmwfAABWbLUBeSR5a1VdWlVnzmWHjzGum99/Osnh8/sjk1zdlr1mLgMAgL3GllUu/91jjGur6l5JLqqqj/aZY4xRVWNPVjgH7TOT5N73vvcqmwcAAHtmVWeQxxjXztMbkrw+ycOSXL8wdGKe3jBXvzbJ0W3xo+ayxes8e4xxwhjjhK1bt66meQAAsMdWHJCr6p5V9Q0L75OclOTDSS5Icvpc7fQkb5jfX5DkKfPdLB6e5JY2FAMAAPYKqxlicXiS11fVwnr+bIzx5qp6b5JXV9XTknwiyZPm+hcmeVySK5PcmuSMVWwbAADWxYoD8hjj40kevET5Z5M8ZonykeSslW4PAAA2gifpAQBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAs+JHTQOwF9i5fbNbsLGO277ZLQD2A84gAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQLNlsxsAALtt5/bNbsHGOm77ZrcA9kvOIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAs2WzGwAALGPn9s1uwcY6bvtmtwCSOIMMAAD/ioAMAACNgAwAAI2ADAAAjYAMAACNgAwAAI2ADAAAjfsgAwB7h53bN7sFG8t9n/daziADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAIyADAEAjIAMAQCMgAwBAs2WzGwAAsF/auX2zW7Cxjtu+2S3YbRt+BrmqTq6qj1XVlVX1rI3ePgAA3JkNDchVdUCSP0xySpJjkzy5qo7dyDYAAMCd2egzyA9LcuUY4+NjjC8lOT/JqRvcBgAAWNZGB+Qjk1zdPl8zlwEAwF5hr7tIr6rOTHLm/PEfqupja7yJw5J8Zo3XydL09cbR1xtHX28M/bxx9PXG2c/7+tc3cmMLfX2flSy80QH52iRHt89HzWX/YoxxdpKz16sBVbVjjHHCeq2fr9LXG0dfbxx9vTH088bR1xtHX2+c1fb1Rg+xeG+SY6rqvlV19ySnJblgg9sAAADL2tAzyGOM26vqZ5K8JckBSc4ZY3xkI9sAAAB3ZsPHII8xLkxy4UZvt1m34Rvcgb7eOPp64+jrjaGfN46+3jj6euOsqq9rjLFWDQEAgLu8DX+SHgAA7M32q4DsMdfrp6qOrqq3VdVlVfWRqnrGXH5oVV1UVVfM00M2u637gqo6oKreX1V/NX++b1W9ez62/3y+CJZVqqqDq+o1VfXRqrq8qh7hmF4fVfXz8/8dH66qV1XV1zqu10ZVnVNVN1TVh1vZksdxTV489/nOqnro5rX8rmeZvv7t+f+QnVX1+qo6uM179tzXH6uqx25Ko++ClurnNu8Xq2pU1WHz5xUd0/tNQPaY63V3e5JfHGMcm+ThSc6a+/dZSS4eYxyT5OL5M6v3jCSXt8//PckLxxj3T/K5JE/blFbte34vyZvHGA9I8uBMfe6YXmNVdWSSn0tywhjjQZku4j4tjuu1cm6SkxeVLXccn5LkmPl1ZpKXbFAb9xXn5o59fVGSB40xjkvyd0menSTz78jTknzbvMwfzVmFXTs3d+znVNXRSU5K8slWvKJjer8JyPGY63U1xrhujPG++f0XMgWJIzP18XlztfOSPGFTGrgPqaqjknx/kpfNnyvJo5O8Zq6in9dAVR2U5N8neXmSjDG+NMa4OY7p9bIlyddV1ZYk90hyXRzXa2KM8fYkNy0qXu44PjXJK8bkXUkOrqojNqSh+4Cl+nqM8dYxxu3zx3dlegZEMvX1+WOM28YYf5/kykxZhV1Y5phOkhcm+eUk/QK7FR3T+1NA9pjrDVJV25I8JMm7kxw+xrhunvXpJIdvVrv2IS/K9B/AV+bP35jk5vYfsGN7bdw3yY1J/mQezvKyqrpnHNNrboxxbZLfyXTW57oktyS5NI7r9bTccex35fr6qSRvmt/r6zVUVacmuXaM8cFFs1bUz/tTQGYDVNXXJ3ltkmeOMT7f543plilum7IKVfX4JDeMMS7d7LbsB7YkeWiSl4wxHpLki1k0nMIxvTbm8a+nZvpS8s1J7pkl/nzK+nAcb4yq+pVMwxFfudlt2ddU1T2SPCfJr63VOvengLzLx1yzOlX1NZnC8SvHGK+bi69f+FPGPL1hs9q3j3hkkh+sqqsyDRN6dKZxsgfPf5pOHNtr5Zok14wx3j1/fk2mwOyYXnvfm+Tvxxg3jjH+OcnrMh3rjuv1s9xx7HflOqiqpyZ5fJIfH1+9v66+Xjv3y/QF+4Pz78ejkryvqr4pK+zn/Skge8z1OprHwb48yeVjjN9tsy5Icvr8/vQkb9jotu1LxhjPHmMcNcbYlukY/usxxo8neVuSH5mr6ec1MMb4dJKrq+rfzEWPSXJZHNPr4ZNJHl5V95j/L1noa8f1+lnuOL4gyVPmK/8fnuSWNhSDFaiqkzMNi/vBMcatbdYFSU6rqgOr6r6ZLiJ7z2a08a5ujPGhMca9xhjb5t+P1yR56Pz/+IqO6f3qQSFV9bhM4zcXHnP9/M1t0b6jqr47yTuSfChfHRv7nEzjkF+d5N5JPpHkSWOMpQbWs4eq6sQk/2WM8fiq+pZMZ5QPTfL+JD8xxrhtE5u3T6iq4zNdDHn3JB9PckamEwuO6TVWVb+e5Mcy/Qn6/Ul+OtM4Qcf1KlXVq5KcmOSwJNcneW6Sv8gSx/H8BeUPMg1xuTXJGWOMHZvQ7LukZfr62UkOTPLZudq7xhj/ca7/K5nGJd+eaWjimxavkztaqp/HGC9v86/KdFecz6z0mN6vAjIAAOzK/jTEAgAAdklABgCARkAGAIBGQAYAgEZABgCARkAGAIBGQAYAgEZABgCA5v8DAcM5mNe+51sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,7.5))\n",
    "plt.title('Train comment Length', fontsize=20)\n",
    "\n",
    "plt.hist(train['comment'].str.len(), alpha=0.5, color='orange')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # 레이아웃 설정\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d2a518d-4cda-40c5-9dd2-9ef560b43208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65373/2562897324.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train['title'] = train['title'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
      "/tmp/ipykernel_65373/2562897324.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test['title'] = test['title'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
      "/tmp/ipykernel_65373/2562897324.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['title'] = test['title'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
      "/tmp/ipykernel_65373/2562897324.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train['comment'] = train['comment'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
      "/tmp/ipykernel_65373/2562897324.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test['comment'] = test['comment'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
      "/tmp/ipykernel_65373/2562897324.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['comment'] = test['comment'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>bias</th>\n",
       "      <th>hate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>미스터 샤인션 변요한 김태리와 같은 양복 입고 학당 방문 이유는</td>\n",
       "      <td>김태리 정말 연기잘해 진짜</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>현장극사실주의 현실가장 보통의 연애 김래원공효진 16년만의 랑데부종합</td>\n",
       "      <td>공효진 발연기나이질생각이읍던데 왜계속주연일까</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>손연재 리듬체조 학원 선생님 하고 싶은 일 해서 행복하다</td>\n",
       "      <td>누구처럼 돈만 밝히는 저급인생은 살아가지마시길 행복은 머니순이 아니니깐 작은거에 감...</td>\n",
       "      <td>others</td>\n",
       "      <td>hate</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>섹션 김해숙 허스토리 촬영 후 우울증 얻었다</td>\n",
       "      <td>일본 축구 져라</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>단독 임현주 아나운서 노브라 챌린지 방송 덕에 낸 용기 자연스런 논의의 창 됐으면 인터뷰</td>\n",
       "      <td>난 절대로 임현주 욕하는인간이랑은 안논다</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                미스터 샤인션 변요한 김태리와 같은 양복 입고 학당 방문 이유는   \n",
       "1             현장극사실주의 현실가장 보통의 연애 김래원공효진 16년만의 랑데부종합   \n",
       "2                    손연재 리듬체조 학원 선생님 하고 싶은 일 해서 행복하다   \n",
       "3                           섹션 김해숙 허스토리 촬영 후 우울증 얻었다   \n",
       "4  단독 임현주 아나운서 노브라 챌린지 방송 덕에 낸 용기 자연스런 논의의 창 됐으면 인터뷰   \n",
       "\n",
       "                                             comment    bias  hate  label  \n",
       "0                                     김태리 정말 연기잘해 진짜    none  none      0  \n",
       "1                           공효진 발연기나이질생각이읍던데 왜계속주연일까    none  hate      1  \n",
       "2  누구처럼 돈만 밝히는 저급인생은 살아가지마시길 행복은 머니순이 아니니깐 작은거에 감...  others  hate      3  \n",
       "3                                           일본 축구 져라    none  none      0  \n",
       "4                            난 절대로 임현주 욕하는인간이랑은 안논다     none  none      0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['title'] = train['title'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
    "test['title'] = test['title'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
    "train['comment'] = train['comment'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
    "test['comment'] = test['comment'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ab64b21d-e5ff-4cc9-be1d-a095025c5f83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.47)\n",
      "Collecting tokenizers!=0.11.3,>=0.10.1\n",
      "  Using cached tokenizers-0.11.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.8.0rc4\n",
      "    Uninstalling tokenizers-0.8.0rc4:\n",
      "      Successfully uninstalled tokenizers-0.8.0rc4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 3.0.0\n",
      "    Uninstalling transformers-3.0.0:\n",
      "      Successfully uninstalled transformers-3.0.0\n",
      "Successfully installed tokenizers-0.11.5 transformers-4.16.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3477b9ec-4e96-436a-b389-25c000cdd1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8fd0ba-0de6-41a7-a6cc-ac1f6abd3e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "306446fe-e484-4a45-9cf3-324d80e7148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed:int = 1004):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a31e46c9-0c10-4897-8a85-0470158f6fc1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/4eb906e7d0da2b04e56c7cc31ba068d7c295240a51690153c2ced71c9e4c9fc5.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/360b579947002f14f22331a026821b56f70679f1be1e95fe5dc5a80edc4a59e0.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/1a24ab4628028ed80dea35ce3334a636dc656fd9a17a09bad377f88f0cbecdac.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8f31ccbd66730704a8400c96db0647b10c47cd0c838ea2cabf0a86ef878f31cf.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fd91c85effc137c99cd14cfe5c3459faa223c005b1577dc2c5aa48f6b2c4fbb1.3d5d467e78cd19d9a87029910ed83289edde0111a75a41e0cc79ad3fc06e4a51\n",
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (18): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (19): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (20): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (21): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (22): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (23): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'klue/roberta-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = 3\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "print(model)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b4b1e27b-5322-446b-83a2-108107267344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,  1443,  2792,  2800,  8855,  2504, 14903,  7952,  2701,  3983,\n",
      "         5180,  2382,  2437,  2205,  2318,  4151,  2371,  2062,  3752,     2,\n",
      "         1405,  2116, 19954, 14036,  4015,  2116,   859,  2259,  2249,  1345,\n",
      "         2181,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1])\n",
      "[CLS] 오또맘 장성규 팔로우 게시글 결국 사과경솔하게 행동했다 전문 [SEP] 얘가 뭔데기사거리가 되는걸 써라 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, eval_dataset = train_test_split(train, test_size=0.2, shuffle=True, stratify=train['label'])\n",
    "\n",
    "tokenized_train = tokenizer(\n",
    "    list(train_dataset['title']),\n",
    "    list(train_dataset['comment']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256, # Max_Length = 140\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "tokenized_eval = tokenizer(\n",
    "    list(eval_dataset['title']),\n",
    "    list(eval_dataset['comment']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "print(tokenized_train['input_ids'][0])\n",
    "print(tokenizer.decode(tokenized_train['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3accda45-4498-4303-bf23-664576af1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pair_dataset, label):\n",
    "        self.pair_dataset = pair_dataset\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "        item['label'] = torch.tensor(self.label[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b71a08b6-190f-445e-83aa-5494941cfa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_num(label):\n",
    "    label_dict = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}\n",
    "    num_label = []\n",
    "\n",
    "    for v in label:\n",
    "        num_label.append(label_dict[v])\n",
    "    \n",
    "    return num_label\n",
    "\n",
    "\n",
    "train_label = label_to_num(train_dataset['label'].values)\n",
    "eval_label = label_to_num(eval_dataset['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b6bd22bc-59ab-4a88-80fc-07b837f7cf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400\n",
      "{'input_ids': tensor([    0,  6040,  2280, 26708,  1510,  2603,  2059, 14691,  3535,  2310,\n",
      "         4829,  4319,  2289,  2118,  2056,  4507,  7703,  2170,  1528,  2536,\n",
      "         2042,     2,  3994,  2073, 30055,  2259,  4369,  2170,  1460,  1176,\n",
      "         2069,  2369,  5124,  6080, 11336,  2052,  2203,  1460,  3737,  4996,\n",
      "         5497,  1077, 10893,  2275,  8174, 22883,  8174,  2348,  2116,  1460,\n",
      "         2052,  2030,  2209,     2,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(3)}\n",
      "[CLS] 이슈강용석 임블리 동거빚투 의혹 제기임지현 남편 반박에 재반박 [SEP] 당신은 잘사는 가정에 왜 불을내 완전 나쁜사람이네 왜 이렇게 살아 나아가 몇살이야 배운만큼 배운애가 왜이러니 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BERTDataset(tokenized_train, train_label)\n",
    "eval_dataset = BERTDataset(tokenized_eval, eval_label)\n",
    "\n",
    "print(train_dataset.__len__())\n",
    "print(train_dataset.__getitem__(3333))\n",
    "print(tokenizer.decode(train_dataset.__getitem__(3333)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "512fe01a-6750-4105-8b21-a0e4f53f0f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "  \"\"\" validation을 위한 metrics function \"\"\"\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  probs = pred.predictions\n",
    "\n",
    "  # calculate accuracy using sklearn's function\n",
    "  acc = accuracy_score(labels, preds) # 리더보드 평가에는 포함되지 않습니다.\n",
    "\n",
    "  return {\n",
    "      'accuracy': acc,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "720a6f06-a274-4115-b18f-6458c3d83702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_65373/475526096.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_ars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_model_on_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "training_ars = TrainingArguments(\n",
    "    # output_dir='/USER/3_WEEK/MNC_NLP/comment_baseline/result',\n",
    "    output_dir='./result',\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_total_limit=5,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 500,\n",
    "    load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_ars,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# CUDA_LAUNCH_BLOCKING=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a67084fc-43d1-45a1-bfa6-1e7dacbd25f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6400\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1400\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_65373/4165317674.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# model.save_pretrained('/USER/3_WEEK/MNC_NLP/comment_baseline/result/best_model')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./result/best_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;31m# tr_loss is a tensor to avoid synchronization of TPUs through .item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m         \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1303\u001b[0m         \u001b[0;31m# _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_loss_scalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "# model.save_pretrained('/USER/3_WEEK/MNC_NLP/comment_baseline/result/best_model')\n",
    "model.save_pretrained('./result/best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4602e738-64a6-488c-9443-19686f92df7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Mon Feb 28 00:52:25 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   63C    P0    30W /  70W |   8644MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a70b83-4893-4be6-9413-85eda20b3ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
