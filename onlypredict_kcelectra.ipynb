{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b3142-27cd-476f-91f2-0f1e73c9bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "import shutil\n",
    "import wandb\n",
    "import gluonnlp as nlp\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from attrdict import AttrDict\n",
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import *\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import math\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from transformers import logging\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import (\n",
    "   get_linear_schedule_with_warmup, \n",
    "   get_cosine_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from transformers import ( \n",
    "    AutoConfig,\n",
    "    BertConfig,\n",
    "    ElectraConfig\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer,  \n",
    "    AutoTokenizer,\n",
    "    ElectraTokenizer,\n",
    "    AlbertTokenizer\n",
    "\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    AutoModel, \n",
    "    ElectraForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    "    AlbertForSequenceClassification,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "# from kobert import get_tokenizer,  get_pytorch_kobert_model\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "print('all libraries imported succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682a2e11-87ba-4304-aced-3e78da23400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 GPU 지정\n",
    "print(\"number of GPUs: \", torch.cuda.device_count())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Does GPU exist? : \", use_cuda)\n",
    "DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # pl.seed_everything(seed)\n",
    "\n",
    "seed_everything(args.seed)\n",
    "print('seed setting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8b558-7678-49e6-8f31-47821e49af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_tokenizer = AutoTokenizer.from_pretrained(\"beomi/beep-KcELECTRA-base-bias\")\n",
    "bias_model = AutoModelForSequenceClassification.from_pretrained(\"beomi/beep-KcELECTRA-base-bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e760fe-348b-4e6e-8bde-b415a26e45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {    \n",
    "    \"run\": \"onlypredict\",\n",
    "    \"data_dir\": \"/USER/comp4/data\",\n",
    "    \"result_dir\": \"/USER/comp4/result/\",\n",
    "    \"config_dir\": \"/USER/comp4/exp_config/\",\n",
    "    \"pretrained_model\": \"beomi/kcbert-large\",\n",
    "    \"architecture\": \"AutoModelForSequenceClassification\",\n",
    "    \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    \"num_classes\": 6,\n",
    "    \"max_seq_len\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa9665-781f-4fbd-b396-fc2531085809",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(os.path.join(args['data_dir'],'test.csv'),encoding = 'UTF-8-SIG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51013f53-1e35-48a3-b781-df9f2d475b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.comment.str.len().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c6ae9-9e1b-40b9-a0d4-5c74f1a4cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "    pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
    "    url_pattern = re.compile(\n",
    "        r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "    x = pattern.sub(' ', x)\n",
    "    x = url_pattern.sub('', x)\n",
    "    x = x.strip()\n",
    "    x = repeat_normalize(x, num_repeats=2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092529ed-a721-4b63-978a-1c74486b078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(ID,title,comment) in tqdm(enumerate(test_df.to_numpy())):\n",
    "    test_df.comment[i] = clean(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa48af2-267b-4284-a493-1bb019013c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = random.randint(1,500)\n",
    "test_df.comment[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85495d0e-034f-468e-acdc-e4e72078988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_num(label):\n",
    "    label_dict = {\"none\": 0, \"others\": 1, \"gender\": 2}\n",
    "    # label_dict = {\"none\": 0, \"gender\": 1, \"others\": 2}\n",
    "    num_label = []\n",
    "\n",
    "    for v in label:\n",
    "        num_label.append(label_dict[v])\n",
    "    \n",
    "    return num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5d358-c357-4b03-9b4d-e303fd03f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['bias'] = \"none\"\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b7f343-af93-4ef7-b48a-694e753afa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bias = label_to_num(test_df.bias.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516463b-fc61-48cd-bcc9-020c1dc14209",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = bias_tokenizer(\n",
    "    # list(train_dataset['title']),\n",
    "    list(test_df['comment']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=args['max_seq_len'], # Max_Length = 190\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1852182-dda9-4989-b47e-f0b0f54717f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class biasDataset(Dataset):\n",
    "    def __init__(self, pair_dataset, bias):\n",
    "        self.pair_dataset = pair_dataset\n",
    "        self.bias = bias\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "        item['bias'] = torch.tensor(self.bias[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd33af0-e73b-4367-9d0b-5f81f8fd562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = biasDataset(tokenized_test, test_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ba6a1-d420-4c82-a89a-1bf45024f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bias_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "bias_model.to(DEVICE)\n",
    "bias_model.eval()\n",
    "output_pred = []\n",
    "output_prob = []\n",
    "\n",
    "for i, data in enumerate(tqdm(bias_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        outputs = bias_model(\n",
    "            input_ids=data['input_ids'].to(DEVICE),\n",
    "            attention_mask=data['attention_mask'].to(DEVICE),\n",
    "            token_type_ids=data['token_type_ids'].to(DEVICE)\n",
    "        )\n",
    "    logits = outputs[0]\n",
    "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits, axis=-1)\n",
    "\n",
    "    output_pred.append(result)\n",
    "    output_prob.append(prob)\n",
    "  \n",
    "pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
    "# print(pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faaaa93-f1d2-41dc-8ee5-a3d5c43fd115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_label(label):\n",
    "    label_dict = {0: \"none\", 1: \"others\", 2: \"gender\"}\n",
    "    str_label = []\n",
    "\n",
    "    for i, v in enumerate(label):\n",
    "        str_label.append([i,label_dict[v]])\n",
    "    \n",
    "    return str_label\n",
    "\n",
    "bias = num_to_label(pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef86d2-64f9-48b8-85fc-fa797434b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_tokenizer = AutoTokenizer.from_pretrained(\"beomi/beep-KcELECTRA-base-hate\")\n",
    "hate_model = AutoModelForSequenceClassification.from_pretrained(\"beomi/beep-KcELECTRA-base-hate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4c63e-093f-4cf5-bb03-2ce533bca1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_num(label):\n",
    "    label_dict = {\"none\": 0, \"offensive\": 1,\"hate\":2}\n",
    "    # label_dict = {\"none\": 0, \"pos\": 1}\n",
    "    num_label = []\n",
    "\n",
    "    for v in label:\n",
    "        num_label.append(label_dict[v])\n",
    "    \n",
    "    return num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e2fcf3-78bd-41f3-83f6-f99502bfdb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['hate'] = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18d9e2-29ed-490d-be6d-d82b3528d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hate = label_to_num(test_df.hate.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69745963-7493-442c-9db4-6523bbd73ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_hate = hate_tokenizer(\n",
    "    # list(train_dataset['title']),\n",
    "    list(test_df['comment']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=args['max_seq_len'], # Max_Length = 190\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184ef5b-9ce7-4471-851a-80254e85d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hateDataset(Dataset):\n",
    "    def __init__(self, pair_dataset, hate):\n",
    "        self.pair_dataset = pair_dataset\n",
    "        self.hate = hate\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "        item['hate'] = torch.tensor(self.hate[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e370f08-335a-4687-9057-7aa4f24d866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_test_dataset = hateDataset(tokenized_test, test_hate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00122cf7-097b-416c-bd96-8976a9fad7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hate_dataloader = DataLoader(hate_test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "hate_model.to(DEVICE)\n",
    "hate_model.eval()\n",
    "output_pred = []\n",
    "output_prob = []\n",
    "\n",
    "for i, data in enumerate(tqdm(hate_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        outputs = hate_model(\n",
    "            input_ids=data['input_ids'].to(DEVICE),\n",
    "            attention_mask=data['attention_mask'].to(DEVICE),\n",
    "            token_type_ids=data['token_type_ids'].to(DEVICE)\n",
    "        )\n",
    "    logits = outputs[0]\n",
    "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits, axis=-1)\n",
    "\n",
    "    output_pred.append(result)\n",
    "    output_prob.append(prob)\n",
    "  \n",
    "pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
    "# print(pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2cbbd7-e0c5-4f0f-bfd6-225a75d4c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_label(label):\n",
    "    label_dict = {0: \"none\", 1: \"offensive\",2:\"hate\"}\n",
    "    # label_dict = {0: \"hate\", 1: \"none\"}\n",
    "    \n",
    "    str_label = []\n",
    "\n",
    "    for i, v in enumerate(label):\n",
    "        str_label.append([i,label_dict[v]])\n",
    "    \n",
    "    return str_label\n",
    "\n",
    "hate = num_to_label(pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d513808-5d44-4517-8a51-2cc6d0c14292",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,val in enumerate(hate):\n",
    "    if hate[i][1] == 'offensive':\n",
    "        hate[i][1] = 'hate'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88403dde-fb5b-4cfb-b676-7c7a4f72bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bias, columns=['ID', 'bias'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a482405-491d-4376-a1f5-5c0932f6a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.DataFrame(hate, columns=['ID', 'hate'])\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4169e-b85e-4c4d-bbb5-4dc008d6882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hate'] = df_2['hate']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4921ee43-8ed8-44a9-b15c-c1190e36f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(args['result_dir'],'onlypredict.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4fc40-6e38-4942-928c-0a54ef276f24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
